{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from os import path\n",
    "import os\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.naive_bayes import GaussianNB # Gaussiana de Naive-Bayes\n",
    "from sklearn.neural_network import MLPClassifier # Multi-layer Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier # K-Nearest Neighbors\n",
    "from sklearn.tree import DecisionTreeClassifier # Decision Tree\n",
    "from sklearn.ensemble import RandomForestClassifier # Random Forest\n",
    "from sklearn.ensemble import AdaBoostClassifier # AdaBoost\n",
    "from sklearn.svm import SVC # Support Vector Machine (Linear, Polynomial, RBF)\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from imblearn.metrics import sensitivity_score\n",
    "from imblearn.metrics import specificity_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"All_Classes_Classification\"\n",
    "\n",
    "# Set datasets to classification: ['Raw', 'Normalized', 'Fourier', 'HOS', 'SCM']\n",
    "datasets_name_list = ['Fourier', 'SCM']\n",
    "\n",
    "# Set classifiers to classification: ['Naive_Bayes','MLP','Nearest_Neighbors','Random_Forest','SVM_Linear','SVM_Polynomial','SVM_RBF']\n",
    "classifiers_name_list = ['Naive_Bayes','MLP','Nearest_Neighbors','Random_Forest','SVM_Linear','SVM_Polynomial','SVM_RBF']\n",
    "\n",
    "# Set metrics to evaluate classifier performance: ['accuracy','balanced_accuracy','precision','sensitivity','specificity','f1_score','fit_time','predict_time']\n",
    "metrics_name_list = ['accuracy','balanced_accuracy','precision','sensitivity','specificity','f1_score','fit_time','predict_time']\n",
    "\n",
    "# Set classes to classification: 3225 = CLASS 0, 3225_REVERSO = CLASS 1, 3225_VAZIO = CLASS 2, 3230 = CLASS 3, 3230_VAZIO = CLASS 4, 3235 = CLASS 5\n",
    "classes = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "# Grouping classes\n",
    "group_classes = False\n",
    "\n",
    "classification_type = 'KFold_Test' # 'KFold_Test_Val', 'KFold_Test', 'HoldOut'\n",
    "\n",
    "# Define the grouping, if it is True\n",
    "if group_classes:\n",
    "    changeClass_dict = {0:1, 2:0, 3:1, 4:0} # format {old:new, old:new,...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making dirs to save results\n",
    "outpath = path.join(\"..\",\"results\",\"classification\",EXPERIMENT_NAME)\n",
    "os.makedirs(outpath)\n",
    "\n",
    "train_path = path.join(outpath, 'Train')\n",
    "os.makedirs(train_path)\n",
    "\n",
    "if classification_type == 'KFold_Test_Val':\n",
    "    validation_path = path.join(outpath, 'Validation')\n",
    "    os.makedirs(validation_path)\n",
    "\n",
    "for ds_name in datasets_name_list:\n",
    "    os.makedirs(path.join(train_path, ds_name))\n",
    "    if classification_type == 'KFold_Test_Val':\n",
    "        os.makedirs(path.join(validation_path, ds_name))\n",
    "    \n",
    "def join_classes(df, dict_groups):\n",
    "    df['Class'].replace(dict_groups, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "datasets_list = []\n",
    "\n",
    "if 'Raw' in datasets_name_list:\n",
    "    RawDs = pd.read_csv(path.join(\"..\",\"results\",\"extraction\",\"32FINAL.csv\"))\n",
    "    RawDs = RawDs.drop(columns=['Tmp'+str(i) for i in range(100)]) # Exclude Tmp columns\n",
    "    datasets_list.append(RawDs)\n",
    "    \n",
    "if 'Normalized' in datasets_name_list:\n",
    "    NormDs = pd.read_csv(path.join(\"..\",\"results\",\"extraction\",\"32FINAL_NORM.csv\"))\n",
    "    NormDs = NormDs.drop(columns=['Tmp'+str(i) for i in range(100)]) # Exclude Tmp columns\n",
    "    datasets_list.append(NormDs)\n",
    "    \n",
    "if 'FFT' in datasets_name_list:  ## NEW\n",
    "    FFTDs = pd.read_csv(path.join(\"..\",\"results\",\"extraction\",\"32FINAL_FFT.csv\"))\n",
    "    datasets_list.append(FFTDs)\n",
    "    \n",
    "if 'Fourier' in datasets_name_list:\n",
    "    FourierDs = pd.read_csv(path.join(\"..\",\"results\",\"extraction\",\"32FINAL_Fourier.csv\"))\n",
    "#     FourierDs = FourierDs.drop(columns=['AcY'+str(i) for i in range(4)])\n",
    "#     FourierDs = FourierDs.drop(columns=['AcZ'+str(i) for i in range(4)])\n",
    "#     FourierDs = FourierDs.drop(columns=['GyX'+str(i) for i in range(4)])\n",
    "#     FourierDs = FourierDs.drop(columns=['GyY'+str(i) for i in range(4)])\n",
    "#     FourierDs = FourierDs.drop(columns=['GyZ'+str(i) for i in range(4)]) \n",
    "    datasets_list.append(FourierDs)\n",
    "    \n",
    "if 'HOS' in datasets_name_list:\n",
    "    HOSDs = pd.read_csv(path.join(\"..\",\"results\",\"extraction\",\"32FINAL_HOS.csv\"))\n",
    "    datasets_list.append(HOSDs)\n",
    "    \n",
    "if 'SCM' in datasets_name_list:\n",
    "    SCMDs = pd.read_csv(path.join(\"..\",\"results\",\"extraction\",\"32FINAL_SCM.csv\"))\n",
    "#     SCMDs = SCMDs.drop(columns=['AcY'+str(i) for i in range(8)])\n",
    "#     SCMDs = SCMDs.drop(columns=['AcZ'+str(i) for i in range(8)])\n",
    "#     SCMDs = SCMDs.drop(columns=['GyX'+str(i) for i in range(8)])\n",
    "#     SCMDs = SCMDs.drop(columns=['GyY'+str(i) for i in range(8)])\n",
    "#     SCMDs = SCMDs.drop(columns=['GyZ'+str(i) for i in range(8)])\n",
    "    datasets_list.append(SCMDs)\n",
    "\n",
    "# Exclude classes that are not in scope \n",
    "for ds in datasets_list:\n",
    "    for i in range(6):\n",
    "        if i not in classes:\n",
    "            indexNames = ds[ds['Class'] == i].index\n",
    "            ds.drop(indexNames, inplace=True) # Delete these row indexes from dataframe\n",
    "\n",
    "# Grouping classes           \n",
    "if group_classes:\n",
    "    for ds in datasets_list:\n",
    "        ds = join_classes(ds, changeClass_dict)\n",
    "    print(\"Experiment Classes: \", classes)\n",
    "    classes = np.unique(list(changeClass_dict.values())).tolist()\n",
    "    print(\"New Classes: \", classes)\n",
    "        \n",
    "unique, counts = np.unique(SCMDs['Class'], return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint\n",
    "\n",
    "# Load classifiers\n",
    "classifiers_list = [GaussianNB(),        \n",
    "                    MLPClassifier(max_iter=1000, solver='adam', learning_rate_init=5e-04),      \n",
    "                    KNeighborsClassifier(),   \n",
    "                    RandomForestClassifier(),\n",
    "                    SVC(kernel='linear', probability=True, tol=1e-3),\n",
    "                    SVC(kernel='poly', probability=True, tol=1e-3),\n",
    "                    SVC(kernel='rbf', probability=True, tol=1e-3)\n",
    "                   ]\n",
    "\n",
    "# param_dist_dict = {'Naive_Bayes': [], \n",
    "#                    'MLP': {\"hidden_layer_sizes\": list(np.arange(2,1001))},\n",
    "#                    'Nearest_Neighbors': {\"n_neighbors\": [1,3,5,7,9,11]}, \n",
    "#                    'Random_Forest': {\"n_estimators\": [3000],\n",
    "#                                      \"max_depth\": [6, None],\n",
    "#                                      \"max_features\": randint(1, 11),\n",
    "#                                      \"min_samples_split\": randint(2, 11),\n",
    "#                                      \"min_samples_leaf\": randint(1, 11),\n",
    "#                                      \"bootstrap\": [True, False],\n",
    "#                                      \"criterion\": [\"gini\", \"entropy\"]},\n",
    "#                     'SVM_Linear': {'kernel': ['linear'], 'C': [2**i for i in range(-5,15)]},\n",
    "#                     'SVM_Polynomial': {'kernel': ['poly'], 'degree': [3, 5, 7 ,9], 'C': [2**i for i in range(-5,15)]},                    \n",
    "#                     'SVM_RBF': {'kernel': ['rbf'], 'gamma': [2**i for i in range(-15,3)],\n",
    "#                                 'C': [2**i for i in range(-5,15)]}\n",
    "#                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "current_milli_time = lambda: int(round(time.time() * 1000))\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if classification_type == 'KFold_Test_Val':\n",
    "    for n_ds, dataset in enumerate(datasets_list):\n",
    "        print(\"processing: \" + datasets_name_list[n_ds] + \" dataset\")\n",
    "        train_ds_path = path.join(train_path, datasets_name_list[n_ds])\n",
    "        validation_ds_path = path.join(validation_path, datasets_name_list[n_ds])\n",
    "\n",
    "        X, y = dataset.iloc[:,:-1], dataset.iloc[:,-1]\n",
    "        X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.2, stratify=y, shuffle=True)\n",
    "\n",
    "        list_final_metrics = []\n",
    "        val_list_final_metrics = []\n",
    "\n",
    "        for n_clf, classifier in enumerate(classifiers_list):\n",
    "            classifier_name = classifiers_name_list[n_clf]\n",
    "            if n_clf != 0:\n",
    "                random_search = RandomizedSearchCV(classifier, param_dist_dict[classifier_name], cv=4, \n",
    "                                                   n_iter=5, scoring='accuracy')\n",
    "                random_search.fit(X_train, y_train)\n",
    "                params = random_search.best_params_\n",
    "                classifier.set_params(**params)\n",
    "\n",
    "            print(\"    processing: \" + classifier_name + \" classifier\")\n",
    "            with open(path.join(train_ds_path, classifier_name+\"_config.txt\"), 'w') as clf_txt:\n",
    "                clf_txt.write(str(classifier))\n",
    "\n",
    "            metrics_dict = dict((k,[]) for k in metrics_name_list)\n",
    "            val_metrics_dict = dict((k,[]) for k in metrics_name_list)\n",
    "\n",
    "            cmx = np.zeros((len(classes),len(classes)))\n",
    "\n",
    "            kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "            print(\"        processing: K-Fold iterations\", end='')\n",
    "\n",
    "            for train_index, test_index in kfold.split(X_train, y_train):\n",
    "                X_train2, X_test = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "                y_train2, y_test = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "                fitStart = current_milli_time() # Start fit chronometer\n",
    "                classifier.fit(X_train2, y_train2)\n",
    "                fit_time = current_milli_time() - fitStart # Stop fit chronometer and save time\n",
    "\n",
    "                predictStart = current_milli_time() # Start predict chronometer\n",
    "                y_predict = classifier.predict(X_test)\n",
    "                predict_time = current_milli_time() - predictStart # Stop predict chronometer and save time\n",
    "\n",
    "                accuracy = accuracy_score(y_test, y_predict)\n",
    "                bal_accuracy = balanced_accuracy_score(y_test, y_predict)\n",
    "                precision = precision_score(y_test, y_predict, average='macro')\n",
    "                sensitivity = sensitivity_score(y_test, y_predict, average='macro')\n",
    "                specificity = specificity_score(y_test, y_predict, average='macro')\n",
    "                f1 = f1_score(y_test, y_predict, average='macro')\n",
    "\n",
    "                cmx += confusion_matrix(y_test, y_predict)\n",
    "\n",
    "                metrics_dict[\"accuracy\"].append(accuracy)\n",
    "                metrics_dict[\"balanced_accuracy\"].append(bal_accuracy)\n",
    "                metrics_dict[\"precision\"].append(precision)\n",
    "                metrics_dict[\"sensitivity\"].append(sensitivity)\n",
    "                metrics_dict[\"specificity\"].append(specificity)\n",
    "                metrics_dict[\"f1_score\"].append(f1)\n",
    "                metrics_dict[\"fit_time\"].append(fit_time)\n",
    "                metrics_dict[\"predict_time\"].append(predict_time)\n",
    "\n",
    "                print('.', end='')\n",
    "\n",
    "            print(' Done!')\n",
    "            cmx_csv = pd.DataFrame(cmx.astype(int), index=classes, \n",
    "                                   columns=classes).to_csv(path.join(train_ds_path, classifier_name+\"_cmx.csv\"), sep=',')\n",
    "\n",
    "            mean_metrics_dict = dict((k+'_mean', np.mean(metrics_dict[k])) for k in metrics_dict.keys())\n",
    "            std_metrics_dict = dict((k+'_std', np.std(metrics_dict[k])) for k in metrics_dict.keys())\n",
    "\n",
    "            final_metrics_dict = dict(mean_metrics_dict.items())\n",
    "            final_metrics_dict.update(std_metrics_dict.items())\n",
    "\n",
    "            list_final_metrics.append(final_metrics_dict)\n",
    "\n",
    "            # VALIDATION OF THE MODEL\n",
    "\n",
    "            val_predictStart = current_milli_time() # Start predict chronometer\n",
    "            val_y_predict = classifier.predict(X_validation)\n",
    "            val_predict_time = current_milli_time() - val_predictStart # Stop predict chronometer and save time\n",
    "\n",
    "            val_accuracy = accuracy_score(y_validation, val_y_predict)\n",
    "            val_bal_accuracy = balanced_accuracy_score(y_validation, val_y_predict)\n",
    "            val_precision = precision_score(y_validation, val_y_predict, average='macro')\n",
    "            val_sensitivity = sensitivity_score(y_validation, val_y_predict, average='macro')\n",
    "            val_specificity = specificity_score(y_validation, val_y_predict, average='macro')\n",
    "            val_f1 = f1_score(y_validation, val_y_predict, average='macro')\n",
    "\n",
    "            val_cmx = confusion_matrix(y_validation, val_y_predict)\n",
    "\n",
    "            val_metrics_dict[\"accuracy\"].append(val_accuracy)\n",
    "            val_metrics_dict[\"balanced_accuracy\"].append(val_bal_accuracy)\n",
    "            val_metrics_dict[\"precision\"].append(val_precision)\n",
    "            val_metrics_dict[\"sensitivity\"].append(val_sensitivity)\n",
    "            val_metrics_dict[\"specificity\"].append(val_specificity)\n",
    "            val_metrics_dict[\"f1_score\"].append(val_f1)\n",
    "            val_metrics_dict[\"fit_time\"].append(np.mean(metrics_dict[\"fit_time\"]))\n",
    "            val_metrics_dict[\"predict_time\"].append(val_predict_time)\n",
    "\n",
    "            val_cmx_csv = pd.DataFrame(val_cmx.astype(int), index=classes, \n",
    "                                       columns=classes).to_csv(path.join(validation_ds_path, classifier_name+\"_cmx.csv\"), sep=',')\n",
    "\n",
    "            val_mean_metrics_dict = dict((k, np.mean(val_metrics_dict[k])) for k in val_metrics_dict.keys())\n",
    "\n",
    "            val_list_final_metrics.append(val_mean_metrics_dict)\n",
    "\n",
    "        metrics_csv = pd.DataFrame(list_final_metrics, \n",
    "                                   index=classifiers_name_list).to_csv(path.join(train_ds_path, \"evaluation_metrics.csv\"), sep=',')\n",
    "\n",
    "        val_metrics_csv = pd.DataFrame(val_list_final_metrics, \n",
    "                                       index=classifiers_name_list).to_csv(path.join(validation_ds_path, \"evaluation_metrics.csv\"), sep=',')\n",
    "\n",
    "    print('\\n\\nFinished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if classification_type == 'KFold_Test':\n",
    "    for n_ds, dataset in enumerate(datasets_list):\n",
    "        print(\"processing: \" + datasets_name_list[n_ds] + \" dataset\")\n",
    "        train_ds_path = path.join(train_path, datasets_name_list[n_ds])\n",
    "\n",
    "        X, y = dataset.iloc[:,:-1], dataset.iloc[:,-1]\n",
    "\n",
    "        list_final_metrics = []\n",
    "\n",
    "        for n_clf, classifier in enumerate(classifiers_list):\n",
    "            classifier_name = classifiers_name_list[n_clf]\n",
    "#             if n_clf != 0:\n",
    "#                 random_search = RandomizedSearchCV(classifier, param_dist_dict[classifier_name], cv=4, \n",
    "#                                                    n_iter=5, scoring='accuracy')\n",
    "#                 random_search.fit(X, y)\n",
    "#                 params = random_search.best_params_\n",
    "#                 classifier.set_params(**params)\n",
    "\n",
    "            print(\"    processing: \" + classifier_name + \" classifier\")\n",
    "            with open(path.join(train_ds_path, classifier_name+\"_config.txt\"), 'w') as clf_txt:\n",
    "                clf_txt.write(str(classifier))\n",
    "\n",
    "            metrics_dict = dict((k,[]) for k in metrics_name_list)\n",
    "\n",
    "            cmx = np.zeros((len(classes),len(classes)))\n",
    "\n",
    "            kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "            print(\"        processing: K-Fold iterations\", end='')\n",
    "\n",
    "            for train_index, test_index in kfold.split(X, y):\n",
    "#                 if n_clf != 0:\n",
    "#                     classifier_aux = classifier.set_params(**params) ############\n",
    "#                 else: \n",
    "#                     classifier_aux = classifier\n",
    "                classifier_aux = classifier\n",
    "\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index] ####\n",
    "                y_train, y_test = y.iloc[train_index], y.iloc[test_index] ####\n",
    "\n",
    "                fitStart = current_milli_time() # Start fit chronometer\n",
    "                classifier_aux.fit(X_train, y_train) #####\n",
    "                fit_time = current_milli_time() - fitStart # Stop fit chronometer and save time\n",
    "\n",
    "                predictStart = current_milli_time() # Start predict chronometer\n",
    "                y_predict = classifier_aux.predict(X_test) #####\n",
    "                predict_time = current_milli_time() - predictStart # Stop predict chronometer and save time\n",
    "                \n",
    "                accuracy = accuracy_score(y_test, y_predict)\n",
    "                bal_accuracy = balanced_accuracy_score(y_test, y_predict)                \n",
    "                precision = precision_score(y_test, y_predict, average='macro')                \n",
    "                sensitivity = sensitivity_score(y_test, y_predict, average='macro')                \n",
    "                specificity = specificity_score(y_test, y_predict, average='macro')                \n",
    "                f1 = f1_score(y_test, y_predict, average='macro')\n",
    "\n",
    "                cmx += confusion_matrix(y_test, y_predict)\n",
    "\n",
    "                metrics_dict[\"accuracy\"].append(accuracy)\n",
    "                metrics_dict[\"balanced_accuracy\"].append(bal_accuracy)\n",
    "                metrics_dict[\"precision\"].append(precision)\n",
    "                metrics_dict[\"sensitivity\"].append(sensitivity)\n",
    "                metrics_dict[\"specificity\"].append(specificity)\n",
    "                metrics_dict[\"f1_score\"].append(f1)\n",
    "                metrics_dict[\"fit_time\"].append(fit_time)\n",
    "                metrics_dict[\"predict_time\"].append(predict_time)\n",
    "\n",
    "                print('.', end='')\n",
    "\n",
    "            print(' Done!')\n",
    "            cmx_csv = pd.DataFrame(cmx.astype(int), index=classes, \n",
    "                                   columns=classes).to_csv(path.join(train_ds_path, classifier_name+\"_cmx.csv\"), sep=',')\n",
    "\n",
    "            mean_metrics_dict = dict((k+'_mean', np.mean(metrics_dict[k])) for k in metrics_dict.keys())\n",
    "            std_metrics_dict = dict((k+'_std', np.std(metrics_dict[k])) for k in metrics_dict.keys())\n",
    "\n",
    "            final_metrics_dict = dict(mean_metrics_dict.items()) \n",
    "\n",
    "            final_metrics_dict.update(std_metrics_dict.items())\n",
    "\n",
    "            list_final_metrics.append(final_metrics_dict)\n",
    "\n",
    "        metrics_csv = pd.DataFrame(list_final_metrics, \n",
    "                                   index=classifiers_name_list).to_csv(path.join(train_ds_path, \"evaluation_metrics.csv\"), sep=',')\n",
    "\n",
    "    print('\\n\\nFinished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if classification_type == \"HoldOut\":\n",
    "    for n_ds, dataset in enumerate(datasets_list):\n",
    "        print(\"processing: \" + datasets_name_list[n_ds] + \" dataset\")\n",
    "        train_ds_path = path.join(train_path, datasets_name_list[n_ds])\n",
    "\n",
    "        X, y = dataset.iloc[:,:-1], dataset.iloc[:,-1]\n",
    "        list_final_metrics = []\n",
    "\n",
    "        for n_clf, classifier in enumerate(classifiers_list):\n",
    "            classifier_name = classifiers_name_list[n_clf]\n",
    "            if n_clf != 0:\n",
    "                random_search = RandomizedSearchCV(classifier, param_dist_dict[classifier_name], cv=4, \n",
    "                                                   n_iter=5, scoring='accuracy')\n",
    "                random_search.fit(X, y)\n",
    "                params = random_search.best_params_\n",
    "                classifier.set_params(**params)\n",
    "\n",
    "            print(\"    processing: \" + classifier_name + \" classifier\")\n",
    "            with open(path.join(train_ds_path, classifier_name+\"_config.txt\"), 'w') as clf_txt:\n",
    "                clf_txt.write(str(classifier))\n",
    "\n",
    "            metrics_dict = dict((k,[]) for k in metrics_name_list)\n",
    "\n",
    "            cmx = np.zeros((len(classes),len(classes)))\n",
    "            \n",
    "            print(\"        processing: Hold-Out iterations\", end='')\n",
    "            \n",
    "            for rnd in range(100):\n",
    "                if n_clf != 0:\n",
    "                    classifier_aux = classifier.set_params(**params)\n",
    "                else: \n",
    "                    classifier_aux = classifier\n",
    "                \n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, shuffle=True)\n",
    "\n",
    "                fitStart = current_milli_time() # Start fit chronometer\n",
    "                classifier_aux.fit(X_train, y_train)\n",
    "                fit_time = current_milli_time() - fitStart # Stop fit chronometer and save time\n",
    "\n",
    "                predictStart = current_milli_time() # Start predict chronometer\n",
    "                y_predict = classifier_aux.predict(X_test)\n",
    "                predict_time = current_milli_time() - predictStart # Stop predict chronometer and save time\n",
    "\n",
    "                accuracy = accuracy_score(y_test, y_predict)\n",
    "                bal_accuracy = balanced_accuracy_score(y_test, y_predict)\n",
    "                precision = precision_score(y_test, y_predict, average='macro')\n",
    "                sensitivity = sensitivity_score(y_test, y_predict, average='macro')\n",
    "                specificity = specificity_score(y_test, y_predict, average='macro')\n",
    "                f1 = f1_score(y_test, y_predict, average='macro')\n",
    "\n",
    "                cmx += confusion_matrix(y_test, y_predict)\n",
    "\n",
    "                metrics_dict[\"accuracy\"].append(accuracy)\n",
    "                metrics_dict[\"balanced_accuracy\"].append(bal_accuracy)\n",
    "                metrics_dict[\"precision\"].append(precision)\n",
    "                metrics_dict[\"sensitivity\"].append(sensitivity)\n",
    "                metrics_dict[\"specificity\"].append(specificity)\n",
    "                metrics_dict[\"f1_score\"].append(f1)\n",
    "                metrics_dict[\"fit_time\"].append(fit_time)\n",
    "                metrics_dict[\"predict_time\"].append(predict_time)\n",
    "\n",
    "                print('.', end='')\n",
    "\n",
    "            print(' Done!')\n",
    "            cmx_csv = pd.DataFrame(cmx.astype(int), index=classes, \n",
    "                                   columns=classes).to_csv(path.join(train_ds_path, classifier_name+\"_cmx.csv\"), sep=',')\n",
    "\n",
    "            mean_metrics_dict = dict((k+'_mean', np.mean(metrics_dict[k])) for k in metrics_dict.keys())\n",
    "            std_metrics_dict = dict((k+'_std', np.std(metrics_dict[k])) for k in metrics_dict.keys())\n",
    "\n",
    "            final_metrics_dict = dict(mean_metrics_dict.items())\n",
    "            final_metrics_dict.update(std_metrics_dict.items())\n",
    "\n",
    "            list_final_metrics.append(final_metrics_dict)\n",
    "\n",
    "        metrics_csv = pd.DataFrame(list_final_metrics, \n",
    "                                   index=classifiers_name_list).to_csv(path.join(train_ds_path, \"evaluation_metrics.csv\"), sep=',')\n",
    "\n",
    "    print('\\n\\nFinished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
