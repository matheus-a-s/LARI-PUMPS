{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from os import path\n",
    "import os\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.naive_bayes import GaussianNB # Gaussiana de Naive-Bayes\n",
    "from sklearn.neural_network import MLPClassifier # Multi-layer Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier # K-Nearest Neighbors\n",
    "from sklearn.tree import DecisionTreeClassifier # Decision Tree\n",
    "from sklearn.ensemble import RandomForestClassifier # Random Forest\n",
    "from sklearn.ensemble import AdaBoostClassifier # AdaBoost\n",
    "from sklearn.svm import SVC # Support Vector Machine (Linear, Polynomial, RBF)\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from imblearn.metrics import sensitivity_score\n",
    "from imblearn.metrics import specificity_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: '../results/classification/Empty_Classification'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5bd3ecf8ca82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Making dirs to save results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0moutpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"..\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"results\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"classification\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mEXPERIMENT_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mds_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets_name_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '../results/classification/Empty_Classification'"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_NAME = \"Empty_Classification\"\n",
    "\n",
    "# Set datasets to classification: ['Raw', 'Normalized', 'Fourier', 'HOS', 'SCM']\n",
    "datasets_name_list = ['Fourier', 'HOS', 'SCM']\n",
    "\n",
    "# Set classifiers to classification: ['Naive_Bayes','MLP','Nearest_Neighbors','Random_Forest','SVM_Linear','SVM_Polynomial','SVM_RBF']\n",
    "classifiers_name_list = ['Naive_Bayes','MLP','Nearest_Neighbors','Random_Forest','SVM_Linear','SVM_Polynomial','SVM_RBF']\n",
    "\n",
    "# Set metrics to evaluate classifier performance: ['accuracy','balanced_accuracy','precision','sensitivity','specificity','f1_score','fit_time','predict_time']\n",
    "metrics_name_list = ['accuracy','balanced_accuracy','precision','sensitivity','specificity','f1_score','fit_time','predict_time']\n",
    "\n",
    "# Set classes to classification: 3225 = CLASS 0, 3225_REVERSO = CLASS 1, 3225_VAZIO = CLASS 2, 3230 = CLASS 3, 3230_VAZIO = CLASS 4, 3235 = CLASS 5\n",
    "classes = [0, 2, 3, 4]\n",
    "\n",
    "# Grouping classes\n",
    "group_classes = True\n",
    "\n",
    "# Define the grouping, if it is True\n",
    "if group_classes:\n",
    "    changeClass_dict = {0:1, 2:0, 3:1, 4:0} # format {old:new, old:new,...}\n",
    "\n",
    "# Making dirs to save results\n",
    "outpath = path.join(\"..\",\"results\",\"classification\",EXPERIMENT_NAME)\n",
    "os.makedirs(outpath)\n",
    "\n",
    "for ds_name in datasets_name_list:\n",
    "    os.makedirs(path.join(outpath, ds_name))\n",
    "    \n",
    "def join_classes(df, dict_groups):\n",
    "    df['Class'].replace(dict_groups, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Classes:  [0, 2, 3, 4]\n",
      "New Classes:  [0, 1]\n",
      "{0: 100, 1: 500}\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "datasets_list = []\n",
    "\n",
    "if 'Raw' in datasets_name_list:\n",
    "    RawDs = pd.read_csv(path.join(\"..\",\"results\",\"extraction\",\"32FINAL.csv\"))\n",
    "    RawDs = RawDs.drop(columns=['Tmp'+str(i) for i in range(100)]) # Exclude Tmp columns\n",
    "    datasets_list.append(RawDs)\n",
    "    \n",
    "if 'Normalized' in datasets_name_list:\n",
    "    NormDs = pd.read_csv(path.join(\"..\",\"results\",\"extraction\",\"32FINAL_NORM.csv\"))\n",
    "    NormDs = NormDs.drop(columns=['Tmp'+str(i) for i in range(100)]) # Exclude Tmp columns\n",
    "    datasets_list.append(NormDs)\n",
    "    \n",
    "if 'Fourier' in datasets_name_list:\n",
    "    FourierDs = pd.read_csv(path.join(\"..\",\"results\",\"extraction\",\"32FINAL_Fourier.csv\"))\n",
    "    datasets_list.append(FourierDs)\n",
    "    \n",
    "if 'HOS' in datasets_name_list:\n",
    "    HOSDs = pd.read_csv(path.join(\"..\",\"results\",\"extraction\",\"32FINAL_HOS.csv\"))\n",
    "    datasets_list.append(HOSDs)\n",
    "    \n",
    "if 'SCM' in datasets_name_list:\n",
    "    SCMDs = pd.read_csv(path.join(\"..\",\"results\",\"extraction\",\"32FINAL_SCM.csv\"))\n",
    "    datasets_list.append(SCMDs)\n",
    "\n",
    "# Exclude classes that are not in scope \n",
    "for ds in datasets_list:\n",
    "    for i in range(6):\n",
    "        if i not in classes:\n",
    "            indexNames = ds[ds['Class'] == i].index\n",
    "            ds.drop(indexNames, inplace=True) # Delete these row indexes from dataframe\n",
    "\n",
    "# Grouping classes           \n",
    "if group_classes:\n",
    "    for ds in datasets_list:\n",
    "        ds = join_classes(ds, changeClass_dict)\n",
    "    print(\"Experiment Classes: \", classes)\n",
    "    classes = np.unique(list(changeClass_dict.values())).tolist()\n",
    "    print(\"New Classes: \", classes)\n",
    "        \n",
    "unique, counts = np.unique(HOSDs['Class'], return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint\n",
    "\n",
    "# Load classifiers\n",
    "classifiers_list = [GaussianNB(),        \n",
    "                    MLPClassifier(max_iter=1000, solver='adam', learning_rate_init=5e-04),      \n",
    "                    KNeighborsClassifier(),   \n",
    "                    RandomForestClassifier(),\n",
    "                    SVC(kernel='linear', probability=True, tol=1e-3),\n",
    "                    SVC(kernel='poly', probability=True, tol=1e-3),\n",
    "                    SVC(kernel='rbf', probability=True, tol=1e-3)\n",
    "                   ]\n",
    "\n",
    "param_dist_dict = {'Naive_Bayes': [], \n",
    "                   'MLP': {\"hidden_layer_sizes\": list(np.arange(2,1001))},\n",
    "                   'Nearest_Neighbors': {\"n_neighbors\": [1,3,5,7,9,11]}, \n",
    "                   'Random_Forest': {\"n_estimators\": [3000],\n",
    "                                     \"max_depth\": [6, None],\n",
    "                                     \"max_features\": randint(1, 11),\n",
    "                                     \"min_samples_split\": randint(2, 11),\n",
    "                                     \"min_samples_leaf\": randint(1, 11),\n",
    "                                     \"bootstrap\": [True, False],\n",
    "                                     \"criterion\": [\"gini\", \"entropy\"]},\n",
    "                    'SVM_Linear': {'kernel': ['linear'], 'C': [2**i for i in range(-5,15)]},\n",
    "                    'SVM_Polynomial': {'kernel': ['poly'], 'degree': [3, 5, 7 ,9], 'C': [2**i for i in range(-5,15)]},                    \n",
    "                    'SVM_RBF': {'kernel': ['rbf'], 'gamma': [2**i for i in range(-15,3)],\n",
    "                                'C': [2**i for i in range(-5,15)]}\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing: Fourier dataset\n",
      "    processing: Naive_Bayes classifier\n",
      "        processing: K-Fold iterations.......... Done!\n",
      "    processing: MLP classifier\n",
      "        processing: K-Fold iterations.......... Done!\n",
      "    processing: Nearest_Neighbors classifier\n",
      "        processing: K-Fold iterations.......... Done!\n",
      "    processing: Random_Forest classifier\n",
      "        processing: K-Fold iterations.......... Done!\n",
      "    processing: SVM_Linear classifier\n",
      "        processing: K-Fold iterations.......... Done!\n",
      "    processing: SVM_Polynomial classifier\n",
      "        processing: K-Fold iterations.......... Done!\n",
      "    processing: SVM_RBF classifier\n",
      "        processing: K-Fold iterations.......... Done!\n",
      "processing: HOS dataset\n",
      "    processing: Naive_Bayes classifier\n",
      "        processing: K-Fold iterations.......... Done!\n",
      "    processing: MLP classifier\n",
      "        processing: K-Fold iterations.......... Done!\n",
      "    processing: Nearest_Neighbors classifier\n",
      "        processing: K-Fold iterations.......... Done!\n",
      "    processing: Random_Forest classifier\n",
      "        processing: K-Fold iterations.......... Done!\n",
      "    processing: SVM_Linear classifier\n",
      "        processing: K-Fold iterations.......... Done!\n",
      "    processing: SVM_Polynomial classifier\n",
      "        processing: K-Fold iterations.......... Done!\n",
      "    processing: SVM_RBF classifier\n",
      "        processing: K-Fold iterations.......... Done!\n",
      "processing: SCM dataset\n",
      "    processing: Naive_Bayes classifier\n",
      "        processing: K-Fold iterations.......... Done!\n",
      "    processing: MLP classifier\n",
      "        processing: K-Fold iterations.......... Done!\n",
      "    processing: Nearest_Neighbors classifier\n",
      "        processing: K-Fold iterations.......... Done!\n",
      "    processing: Random_Forest classifier\n",
      "        processing: K-Fold iterations.......... Done!\n",
      "    processing: SVM_Linear classifier\n",
      "        processing: K-Fold iterations.......... Done!\n",
      "    processing: SVM_Polynomial classifier\n",
      "        processing: K-Fold iterations.......... Done!\n",
      "    processing: SVM_RBF classifier\n",
      "        processing: K-Fold iterations.......... Done!\n",
      "\n",
      "\n",
      "Finished!\n",
      "CPU times: user 24min 37s, sys: 8min 29s, total: 33min 7s\n",
      "Wall time: 15min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "current_milli_time = lambda: int(round(time.time() * 1000))\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for n_ds, dataset in enumerate(datasets_list):\n",
    "    print(\"processing: \" + datasets_name_list[n_ds] + \" dataset\")\n",
    "    ds_outpath = path.join(outpath, datasets_name_list[n_ds])\n",
    "    \n",
    "    X, y = dataset.iloc[:,:-1], dataset.iloc[:,-1]\n",
    "    X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.2, stratify=y, shuffle=True)\n",
    "    \n",
    "    list_final_metrics = []\n",
    "\n",
    "    for n_clf, classifier in enumerate(classifiers_list):\n",
    "        classifier_name = classifiers_name_list[n_clf]\n",
    "        if n_clf != 0:\n",
    "            random_search = RandomizedSearchCV(classifier, param_dist_dict[classifier_name], cv=4, \n",
    "                                               n_iter=5, scoring='accuracy')\n",
    "            random_search.fit(X_train, y_train)\n",
    "            params = random_search.best_params_\n",
    "            classifier.set_params(**params)\n",
    "            \n",
    "        print(\"    processing: \" + classifier_name + \" classifier\")\n",
    "        with open(path.join(ds_outpath, classifier_name+\"_config.txt\"), 'w') as clf_txt:\n",
    "            clf_txt.write(str(classifier))\n",
    "        \n",
    "        metrics_dict = dict((k,[]) for k in metrics_name_list)\n",
    "        cmx = np.zeros((len(classes),len(classes)))\n",
    "        \n",
    "        kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "        print(\"        processing: K-Fold iterations\", end='')\n",
    "        \n",
    "        for train_index, test_index in kfold.split(X_train, y_train):\n",
    "            X_train2, X_test = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "            y_train2, y_test = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "            \n",
    "            fitStart = current_milli_time() # Start fit chronometer\n",
    "            classifier.fit(X_train2, y_train2)\n",
    "            fit_time = current_milli_time() - fitStart # Stop fit chronometer and save time\n",
    "            \n",
    "            predictStart = current_milli_time() # Start predict chronometer\n",
    "            y_predict = classifier.predict(X_test)\n",
    "            predict_time = current_milli_time() - predictStart # Stop predict chronometer and save time\n",
    "            \n",
    "            accuracy = accuracy_score(y_test, y_predict)\n",
    "            bal_accuracy = balanced_accuracy_score(y_test, y_predict)\n",
    "            precision = precision_score(y_test, y_predict, average='macro')\n",
    "            sensitivity = sensitivity_score(y_test, y_predict, average='macro')\n",
    "            specificity = specificity_score(y_test, y_predict, average='macro')\n",
    "            f1 = f1_score(y_test, y_predict, average='macro')\n",
    "            \n",
    "            cmx += confusion_matrix(y_test, y_predict)\n",
    "            \n",
    "            metrics_dict[\"accuracy\"].append(accuracy)\n",
    "            metrics_dict[\"balanced_accuracy\"].append(bal_accuracy)\n",
    "            metrics_dict[\"precision\"].append(precision)\n",
    "            metrics_dict[\"sensitivity\"].append(sensitivity)\n",
    "            metrics_dict[\"specificity\"].append(specificity)\n",
    "            metrics_dict[\"f1_score\"].append(f1)\n",
    "            metrics_dict[\"fit_time\"].append(fit_time)\n",
    "            metrics_dict[\"predict_time\"].append(predict_time)\n",
    "            \n",
    "            print('.', end='')\n",
    "            \n",
    "        print(' Done!')\n",
    "        cmx_csv = pd.DataFrame(cmx.astype(int), index=classes, \n",
    "                               columns=classes).to_csv(path.join(ds_outpath, classifier_name+\"_cmx.csv\"), sep=',')\n",
    "        \n",
    "        mean_metrics_dict = dict((k+'_mean', np.mean(metrics_dict[k])) for k in metrics_dict.keys())\n",
    "        std_metrics_dict = dict((k+'_std', np.std(metrics_dict[k])) for k in metrics_dict.keys())\n",
    "        \n",
    "        final_metrics_dict = dict(mean_metrics_dict.items())\n",
    "        final_metrics_dict.update(std_metrics_dict.items())\n",
    "        \n",
    "        list_final_metrics.append(final_metrics_dict)\n",
    "    \n",
    "    metrics_csv = pd.DataFrame(list_final_metrics, \n",
    "                               index=classifiers_name_list).to_csv(path.join(ds_outpath, \"evaluation_metrics.csv\"), sep=',')\n",
    "    \n",
    "print('\\n\\nFinished!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
